do_train = true
do_eval = true
do_test = true
# dataset_names = ["qnli_text", "mnli_text", "dbpedia_text", "trec_coarse_text", "sst2_text", "yelp_polarity_text"]
dataset_names = ["dbpedia_text"]
model_name_or_path = "meta-llama/Meta-Llama-3-8B"
data_tokenizer_name_or_path = "meta-llama/Meta-Llama-3-8B"
max_target_length = 128
per_device_eval_batch_size = 16
per_device_train_batch_size = 16
report_to = ["wandb", "codecarbon"]
wandb_project = "arithmetics"
max_source_length = 256
split_validation_test = true
output_dir = "saves/prompt_tuning"
predict_with_generate = true
evaluation_strategy = "steps"
save_strategy = "steps"
logging_strategy = "steps"
eval_steps = 100
save_steps= 100
logging_steps = 100
load_best_model_at_end = true
# metric_for_best_model = "eval_average_loss"
# greater_is_better = false
save_total_limit = 1
task_type = "CAUSAL_LM"
num_virtual_tokens = 100
weight_decay = 1e-5
warmup_steps = 500
num_train_epochs = 10
learning_rate = 0.03
origin_prompts = ["origin_0_meta-llama-3-8b", "origin_1_meta-llama-3-8b", "origin_2_meta-llama-3-8b"]
bf16=true
pad_to_max_length=true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 500