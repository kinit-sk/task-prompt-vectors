do_train = true
do_eval = true
dataset_names = ["qnli_text_instruct"]
model_name_or_path = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
data_tokenizer_name_or_path = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
max_seq_length = 512
per_device_train_batch_size = 4
report_to = ["wandb"]
split_validation_test = true
output_dir = "saves/prompt_tuning"
eval_strategy = "steps"
save_strategy = "steps"
logging_strategy = "steps"
eval_steps = 0.1
save_steps= 0.1
logging_steps = 1
load_best_model_at_end = true
save_total_limit = 1
task_type = "CAUSAL_LM"
num_virtual_tokens = 100
weight_decay = 1e-5
warmup_ratio = 0.03
num_train_epochs = 10
learning_rate = 0.3
origin_prompts = ["origin_2_meta-llama-3.1-8b-instruct"]
bf16=true
# max_train_samples = 1000
# max_valid_samples = 500
# max_test_samples = 100
lr_scheduler_type = "cosine"
optim = "adamw_torch"
group_by_length = false
dataset_text_field="text"